#  SQL/NoSQL

## 关系型数据库

>建立在关系型数据模型（数据间的存在一对一、一对多、多对多的联系）基础上，借助集合代数等数学概念和方法处理数据的数据库，数据会存放在具体的某张表中
>

### ACID

1. **原子性**（Atomicity）：事务的所有操作要么全部提交成功，要么全部提交失败。换言之，事务中的任何一条语句的执行失败，会让所有已经成功执行的事务也要回滚。MySQL实现利用innodb的undo log，其记录回滚所需要的所有信息，可以通过它实现事务的回滚。
2. **一致性**（Consistency）：保证数据从一种一致性状态转移到另一种一致性状态，即事务前后的数据必须保持一致性。比如转账业务，无论事务是否成功，转账的和收款的人的总金额不变。需要通过原子性、隔离性、持久性来保证一致性。AID是手段，C才是目的！
3. **隔离性**（Isolation）：一个事务所作的修改在最终提交以前，对其他事务不可见。MySQL通过锁和MVCC机制实现隔离性（ps：MVCC只能实现提交读和可重复读这两种隔离级别）
4. **持久性**（Durability）：一旦事务提交，则其所作的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。MySQL实现利用innodb的redo log，数据修改时会在redo log中记录这次操作。事务提交时，会将redo log日志写入磁盘（redo log部分在内存中，部分在磁盘上）

### 数据查询流程

以MySQL为例

#### 查询语句执行

> 比较简单，不涉及日志等操作嘞

1. 检查语句是否有权限，如果没有权限会返回错误信息。如果有权限，但MySQL没有或未开启查询缓存功能，直接进行下一步。否则会先以这条SQL语句为key在内存中查询是否有结果，如果有结果直接返回，没有再进行下一步

2. 进行词法分析，提取关键字。比如select、表名、 查询条件；进行语法分析，检查关键词是否正确，语句是否有语法错误。没问题进入下一步

3. 优化器确定执行方案，以下面这样一条SQL语句为例

	```mysql
	select * from tb_student  A where A.age='18' and A.name=' 张三 ';
	```

	有两种执行方法

	1. 先查询年龄18岁的学生，再判断名字是否是张三
	2. 先查询名字是张三的学生，再判断是否18岁

	根据索引或一些情况的不同，优化器会根据自己的优化算法选择执行效率最好的一个方案

4. 检验是否有使用存储引擎的权限，如果有会调用存储引擎接口，并返回执行结果

#### 更新语句执行

> 会涉及到binlog、redo log

1. 查询到对应数据
2. 更改对应字段，调用引擎API接口，重新写入数据。InnoDB引擎会将数据写入内存，并记录在redo log。此时redo log 进入 prepare状态，并通知执行器执行
3. 执行器收到通知后记录binlog，调用引擎接口，提交 redo log为提交状态
4. 更新完成

**一个问题，为什么redo log要引入预提交状态？·**

答案：实现数据的一致性

## 非关系型数据库

> 相较于关系型数据库，NoSQL有如下的特征：

1. 非关系：没有SQL语言，不遵从关系模型等
2. 动态架构：没有事先设置数据库，随时存储
3. API简单：通常用REST(HTTP协议+JSON数据)，提供简洁的界面来存储、查询等
4. 分布式：可以分布式执行多个NoSQL数据库

**PS**：这里的第四个特征分布式，引出NoSQL无法严格满足ACID，接下来介绍BASE原则

### BASE

相较于关系型数据库，非关系型数据库在一定程度上牺牲了ACID。比如更新不必加锁（数据之间没有关系），查询走缓存（准实时的数据）等。

BASE根据CAP规则演变而来，是NoSQL数据库通常对可用性及一致性的弱要求原则：

1. **基本可用(Basically Available)**：即分布式系统出现故障时，保证核心可用，允许损失部分可用性
2. **软状态(Soft State)**：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在延时。
3. **最终一致性(Eventually Consistency)：**系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态

总的来说BASE要实现的是最终一致性，而ACID要求强一致性

### 类别

按存储类型进行分类

| 类型        | 代表                   | 特点                                                         |
| ----------- | ---------------------- | ------------------------------------------------------------ |
| 列存储      | Hbase                  | 方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。 |
| 文档存储    | MongoDB、ElasticSearch | 以json格式存储数据，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能。 |
| key-val存储 | Redis                  | 可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。 |

# 流行数据库

## MySQL

### 基本架构

总的来说，主要分为Server层和存储引擎层：

- **Server 层**：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binlog 日志模块。
- **存储引擎**： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。**现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5 版本开始就被当做默认存储引擎了**。

一图流如下：

![img](笔记.assets\mysql基本架构.png)

1. 连接器：主要负责用户登录数据库，进行用户的身份认证
2. 查询缓存：主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集(实际上不太好用，因为更新一个表会导致所有查询缓存都被清空)
3. 分析器：要是用来分析 SQL 语句是来干嘛的
	1. 词法分析：提取关键字
	2. 语法分析：判断SQL是否正确，是否符合语法
4. 优化器：选取规则认为最优的执行方案去执行
5. 执行器：选择执行方案后调用存储引擎接口，返回接口执行的结果
6. 存储引擎：提供读写的接口

### 并发一致性问题

1. **丢失修改**：T1和T2两个事务都对一个数据进行修改，T1先修改，T2随后修改，T2的修改覆盖了T1的修改。
2. **读脏数据**：T1修改一个数据，T2随后读取这个数据。如果T1撤销了这次修改，那么T2读取的数据是脏数据。
3. **不可重复读**：T2读取一个数据，T1对该数据做了修改。如果T2再次读取这个数据，此时读取的结果和第一次读取的结果不同。
4. **幻影读**：T1读取某个范围的数据，T2在这个范围内插入新的数据，T1再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。

### 封锁

#### 封锁粒度

> 1. **行级锁**：给当前操作的行上锁，并发度高但可能出现死锁。
> 2. **表级锁**：给整张表上锁，并发度比较低但不会出现死锁。

行锁为什么会造成死锁呢？

盲猜一手，可以重复的申请行锁，但是行锁没有设计可重入，所以会死锁

#### 封锁类型

1. **读写锁**：分为读锁（S锁或者共享锁）、写锁（X锁或者排它锁）。概括一下就是读锁写锁互斥，写锁写锁互斥，但读锁读锁不会互斥。

   > 事务给数据对象加写锁可以获取读取和更新的权力，加读锁只能进行读取操作。(Java实现有ReentrantReadWriteLock)

2. **意向锁**：在读写锁基础上引入的，引入了IX|IS两种表锁，表明打算给数据行加上X锁|S锁嘞。在引入意向锁后，获取X|S锁之前，必须获取IX|IS或者其它更强的锁。这个也是比较好判断的，获取X锁需要IX锁，获取S锁需要IS锁或者IX锁。这里讲一下各种锁之间的兼容关系：

| -    | X    | IX   | S    | IS   |
| ---- | ---- | ---- | ---- | ---- |
| X    | ×    | ×    | ×    | ×    |
| IX   | ×    | √    | ×    | √    |
| S    | ×    | ×    | √    | √    |
| IS   | ×    | √    | √    | √    |

- 任意 IS/IX 锁之间都是兼容的，因为它们只是表示想要对表加锁，而不是真正加锁；
- S 锁只与 S 锁和 IS 锁兼容，也就是说事务 T 想要对数据行加 S 锁，其它事务可以已经获得对表或者表中的行的 S 锁。

#### 封锁协议

1. 一级封锁协议，事务T修改数据时必须加X锁，直到T结束后才释放锁：解决丢失修改问题
2. 二级封锁协议：在一级封锁协议基础上，事务T读取数据时必须加S锁，读取完马上释放S锁：解决读脏数据问题（S锁拿着的时候，不准其它事务回滚
3. 三级封锁协议：在二级封锁协议基础上，要求事务读取数据时必须加S锁，直到事务结束后才能释放S锁：解决不可重复读问题不管读几次都不让其它事务操作了
4. 两段锁协议：加锁和解锁分为两个阶段执行：保证可串行化的充分条件，但非必要条件

### 隔离级别

> MySQL默认的隔离级别是可重复读

1. 未提交读：事务中的修改，即使没有提交，对其它事务也是可见的
2. 提交读：一个事务只能读取已经提交的事务所作的修改。
3. 可重复读：**一个**事务中多次读取同样的数据的结果是一样的。
4. 可串行化：强制事务串行执行。

#### 并发一致性问题对应隔离级别

| 隔离级别 | 脏读  | 不可重复读 | 幻影读 |
| ---- | --- | ----- | --- |
| 未提交读 | √   | √     | √   |
| 提交读  | ×   | √     | √   |
| 可重复读 | ×   | ×     | √   |
| 可串行化 | ×   | ×     | ×   |

### 存储引擎

> 初步了解可以分为两种：InnoDDB和MyISAM、MEMORY（仅了解是将表中数据存储在内存上，提供快速访问

MyISAM：设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作的时候才可以使用。！**不支持事务、行级锁**

InnoDB：只有在**需要它不支持的特性时才考虑其它存储引擎**。它实现四个标准的隔离级别，**通过MVCC＋Next-Key Locking（Record lock：记录锁，单个行记录上的锁；Gap lock：间隙锁，锁定一个范围，不包括记录本身；而Next-key lock：由record+gap组成 临键锁，锁定一个范围，包含记录本身）防止幻影读问题。**

主索引是聚簇索引，内部优化有如插入缓冲区（加速插入操作）、自适应哈希索引（加快读操作且是自动创建）、可预测性读（加速磁盘读取数据，这里多说两句：为减少磁盘I//O，磁盘往往不会严格按需读取，每次都会预读，即使只需要一个字节，磁盘也会读取一定长度的数据放入内存）。真正支持在线热备份！！！（热备份是系统处于正常运转状态下的备份。

**比较一下：**

- 事务: InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。
- 并发: MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。
- 外键: InnoDB 支持外键。
- 备份: InnoDB 支持在线热备份。
- 崩溃恢复: MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。
- 其它特性: MyISAM 支持压缩表和空间数据索引。

### 索引

[B+Tree原理以及MySQL的索引分析](https://www.cnblogs.com/xiaoxi/p/6894610.html)

#### 索引的本质

> 在关系型数据库中，索引是一种**单独的、物理的对数据库表中一列或多列的值进行排序的一种存储结构**，它是某个表中一列或若干列值的集合和相应的指向表中物理标识这些值的数据页的逻辑指针清单
>
> 可以用作索引的数据结构有：数组、链表、哈希、红黑树、B树等

MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构（B+树），为提高查询效率而生，

其实现方式上会**以某种方式引用（指向）数据**，并且利用一些数据结构的特性实现高级查找方法（如二分）。

#### 使用索引的原则

**一般来说，索引本身较大，不会存储在内存中，因而会以索引文件的形式存储在磁盘上。随之而来的问题是，索引查找过程中会产生磁盘I/O消耗，相对于内存获取，I/O获取的消耗要高几个数量级。索引的结构组织要尽可能减少磁盘查找过程中的I/O次数。（以B+Tree为例，一页对应一次I/O，降低树的高度就减少了I/O次数）**

#### B-Tree（平衡多路查找树）

B-Tree是为磁盘等外存储设备设计的一种平衡查找树。因此在讲B-Tree之前先了解下磁盘的相关知识。

系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么。

**InnoDB存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位。**InnoDB存储引擎中默认每个页的大小为**16KB**，可通过参数innodb_page_size将页的大小设置为4K、8K、16K，在MySQL中可通过如下命令查看页的大小：

```
mysql> show variables like 'innodb_page_size';
```

而系统一个磁盘块的存储空间往往没有这么大，因此InnoDB每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小16KB。InnoDB在把磁盘数据读入到磁盘时会以页为基本单位，在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘I/O次数，提高查询效率。

B-Tree结构的数据可以让系统高效的找到数据所在的磁盘块。为了描述B-Tree，首先定义一条记录为一个二元组[key, data] ，key为记录的键值，对应表中的主键值，data为一行记录中除主键外的数据。对于不同的记录，key值互不相同。

> 一棵m阶的B-Tree有如下特性：
>
> 1. 每个节点最多有m个孩子。
> 2. 除了根节点和叶子节点外，其它每个节点至少有Ceil(m/2)个孩子。
> 3. 若根节点不是叶子节点，则至少有2个孩子。
> 4. 所有叶子节点都在同一层，且不包含其它关键字信息。
> 5. 每个非终端节点包含n个关键字信息（P0,P1,…Pn, k1,…kn）
> 6. 关键字的个数n满足：ceil(m/2)-1 <= n <= m-1。
> 7. ki(i=1,…n)为关键字，且关键字升序排序。
> 8. Pi(i=1,…n)为指向子树根节点的指针。P(i-1)指向的子树的所有节点关键字均小于ki，但都大于k(i-1)。

以一个3阶的B-Tree为例：

每个节点占用一个盘块的磁盘空间，一个节点上有两个升序排序的关键字和三个指向子树根节点的指针，指针存储的是子节点所在磁盘块的地址。两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。以根节点为例，关键字为17和35，P1指针指向的子树的数据范围为小于17，P2指针指向的子树的数据范围为17~35，P3指针指向的子树的数据范围为大于35。

模拟查找关键字29的过程：

1. 根据根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】
2. 比较关键字29在区间（17,35），找到磁盘块1的指针P2。
3. 根据P2指针找到磁盘块3，读入内存。【磁盘I/O操作第2次】
4. 比较关键字29在区间（26,30），找到磁盘块3的指针P2。
5. 根据P2指针找到磁盘块8，读入内存。【磁盘I/O操作第3次】
6. 在磁盘块8中的关键字列表中找到关键字29。

#### B+Tree

B+Tree是在B-Tree基础上的一种优化，使其更适合实现外存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构。

从上一节中的B-Tree结构图中可以看到每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。

B+Tree相对于B-Tree有几点不同：

1. 非叶子节点只存储键值信息。
2. 所有叶子节点之间都有一个链指针。（）
3. 数据记录都存放在叶子节点中。

通常在B+Tree上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。因此可以对B+Tree进行两种查找运算：一种是对于主键的范围查找和分页查找，另一种是从根节点开始，进行随机查找。

#### 索引介绍

> 首先要说的是，索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。

1. **B+Tree索引**：MySQL普遍使用B+Tree作为索引，具体又可以分为聚簇索引和非聚簇索引，前者主要用于InnoDB存储引擎中，后者主要用于MyISAM存储引擎中。非聚簇索引的B+树叶子节点上的data不是数据本身而是数据的地址，聚簇索引的B+树叶子节点上的data就是数据本身。
2. **哈希索引**：能以O(1)时间进行精确查找，但因为不再有序，所以无法用于部分查找和范围查找。InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。
3. 全文索引：MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。查找条件使用 MATCH AGAINST，而不是普通的 WHERE。全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。
4. 空间数据索引：MyISAM 存储引擎支持空间数据索引(R-Tree)，可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。必须使用 GIS 相关的函数来维护数据。

> 补充：索引覆盖
>
> 索引覆盖是指如果查询的列恰好是索引的一部分，那么查询只需要在索引文件上进行，不需要回行到磁盘再找数据。这种查询速度非常快，称为“索引覆盖”。**走覆盖索引可以避免索引失效**

#### 索引优化

##### 1.独立的列

在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。

例如下面的查询不能使用 actor_id 列的索引:

```sql
SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;
```

##### 2.多列索引

在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。

```sql
SELECT film_id, actor_ id FROM sakila.film_actor
WHERE actor_id = 1 AND film_id = 1;
```

##### 3.索引列的顺序

让选择性最强的索引列放在前面，索引的选择性是指: 不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。

例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。

```sql
SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity, COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity, COUNT(*) FROM payment; 
```

```shell
   staff_id_selectivity: 0.0001 customer_id_selectivity: 0.0373               COUNT(*): 16049 
```

##### 4.前缀索引

对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。

对于前缀长度的选取需要根据索引选择性来确定。

##### 非聚簇索引一定回表查询吗？

> 回表查询指查到索引对应的指针或主键后，仍然需要根据主键或者指针再到数据文件或者表中查询
>
> 非聚簇索引是相对聚簇索引的概念。一般来说聚簇索引就是主键索引，叶子节点上会存储真正的数据。而非聚簇索引的叶子节点上存储的是主键ID

不一定，如果索引本身的key就是查询的内容，查到key后就直接返回，不再需要回表查询。这里引出覆盖索引的概念：**即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。**

#### 最左匹配

对于复合索引，在查询使用时，最好将条件顺序按找索引的顺序，这样效率最高，例如：select * from table1 where col1=A AND col2=B AND col3=D。如果使用 where col2=B AND col1=A 或者 where col2=B 将不会使用索引。

mysql创建复合索引的规则是首先会对复合索引的最左边的，也就是第一个name字段的数据进行排序，在第一个字段的排序基础上，然后再对后面第二个的age字段进行排序。
其实就相当于实现了类似 order by name age这样一种排序规则。
所以：第一个name字段是绝对有序的，而第二字段就是无序的了。所以通常情况下，直接使用第二个age字段进行条件判断是用不到索引的， 这就是所谓的mysql为什么要强调最左前缀原则的原因。

那么什么时候才能用到age字段的索引呢? 前提当然是age字段的索引数据也是有序的情况下才能使用咯，什么时候才是有序的呢？观察可知，是在name字段是等值匹配的情况下，age才是有序的。
这也就是mysql索引规则中要求复合索引要想使用第二个索引，必须先使用第一个索引的原因，而且第一个索引必须是等值匹配。

#### 索引失效

1. 索引的字段类型不匹配：字段类型不匹配时，MySQL会进行隐式的类型转换，即使用内置函数
2. 索引字段使用表达式计算：需要将索引字段取出进行表达式条件判断，导致全表扫描，索引失效
3. like 使用 %X 模糊匹配：使用右模糊匹配还可以走查询，因为左模糊匹配违法最左匹配原则
4. 索引字段不是最左字段：如果索引是abc，单独对b进行查询，不走索引
5. or条件两端有一端没走索引：必须要两端都走索引，才会整体走索引
6. in、not in 可能导致索引失效，由表的数据量决定

### 日志

`MySQL` 日志 主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。其中，比较重要的还要属二进制日志 `binlog`（归档日志）和事务日志 `redo log`（重做日志）和 `undo log`（回滚日志）。

#### redo log（重做日志）

> **实现持久性**：主要是崩溃恢复。ps：InnoDB独有

`MySQL`数据以页为单位，查询一条记录时，会从硬盘加载一页数据，加载出来的数据叫数据，并放入`Buffer Pool`中。

后续的查询都是先从 `Buffer Pool` 中找，没有命中再去硬盘加载，减少硬盘 `IO` 开销，提升性能。

更新表数据的时候，也是如此，发现 `Buffer Pool` 里存在要更新的数据，就直接在 `Buffer Pool` 里更新。

然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（`redo log buffer`）里，接着刷盘到 `redo log` 文件里。

`InnoDB` 存储引擎为 `redo log` 的刷盘策略提供了 `innodb_flush_log_at_trx_commit` 参数（默认为1，它支持三种策略：

- **0** ：设置为 0 的时候，表示每次事务提交时不进行刷盘操作
- **1** ：设置为 1 的时候，表示每次事务提交时都将进行刷盘操作（默认值）
- **2** ：设置为 2 的时候，表示每次事务提交时都只把 redo log buffer 内容写入 page cache

InnoDB存储引擎有一个后台线程，每隔一秒就会把redo log buffer中的内容写到文件缓存page cache中，调用fsync刷盘。（代表没有提交事务的redo log也可能会进行刷盘操作

当`innodb_flush_log_at_trx_commit`参数为0时，MySQL实例挂了或宕机可能会有1s的数据丢失；为1时，则不会丢失数据，因为事务提交了就进行刷盘操作，没提交的事务日志丢失也不会有损失

redo log的日志文件不止一个，而是有一个**日志文件组**的形式出现的，每个的redo日志文件大小都是一样的。它采用的是环形数组形式，从头开始写，写到末尾又回到头循环写，如下图所示。

在个**日志文件组**中还有两个重要的属性，分别是 `write pos、checkpoint`

- **write pos** 是当前记录的位置，一边写一边后移
- **checkpoint** 是当前要擦除的位置，也是往后推移

#### binlog（归档日志）

> **实现持久性**：主要是同步数据，保证集群架构的数据一致性。可实现数据备份、主备、主主、主从。

会在执行更新语句的时候将语句写入binlog，其格式为二进制。

**记录模式：**

1. statement：记录内容为SQL原文，有比如`update_time=now()`获取当前系统时间，直接执行会导致与原库的数据不一致的问题。这里引入row
2. row：记录内容看不见详细信息，需要通过mysqlbinlog工具解析。这里update_time=now()会指向具体的时间，解决了这个问题。
3. mixed：折中方案，两种混合。由`MySQL`判断会不会SQL语句是否会引起数据不一致，再采用对应格式。

**两阶段提交：**

这个概念与是与redo log有关的。执行更新语句时，redo log和binlog两块日志都会记录。但是redo log可以不断写入，binlog只在提交事务时写入（多提一嘴，**不一定每次提交事务都有执行fsync刷盘操作**，可能只会写到page cache中）。为解决两份日志不同写入时机可能导致的逻辑一致问题，InnoDB将redo log的写入拆成prepare和commit两个步骤（就是对redo log中的事务设置了两个阶段属性）

大体上就是redolog prepare -> binlog -> redolog commit

使用**两阶段提交**后，写入`binlog`时发生异常也不会有影响，因为`MySQL`根据`redo log`日志恢复数据时，发现`redo log`还处于`prepare`阶段，并且没有对应`binlog`日志，就会回滚该事务。而`redo log`设置`commit`阶段发生异常，并不会回滚事务，它会执行上图框住的逻辑，虽然`redo log`是处于`prepare`阶段，但是能通过事务`id`找到对应的`binlog`日志，所以`MySQL`认为是完整的，就会提交事务恢复数据。

#### undo log（回滚日志）

> **实现原子性**：事务执行rollback时，可以从undo log中读取到相应内容进行回滚；提供MVCC（多版本并发控制），MVCC 使用到的快照存储在 Undo 日志中，该日志通过回滚指针把一个数据行(Record)的所有快照连接起来。

保证事务的原子性，就需要在异常发生时，对已经执行的操作进行**回滚**，在 MySQL 中，恢复机制是通过 **回滚日志（undo log）** 实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用 **回滚日志** 中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。剩下的具体在MVCC中细🔒。

### MVCC

> 目的是实现非阻塞并发读，以更好方式处理**读写冲突**。！注意，不能解决**修改丢失**问题

**当前读**：select * from table where ? lock in share mode; select * from table where ? for update; insert; update; delete;这些操作是当前读操作，使用的是记录的最新版本，需要保证其他并发事务不能修改当前记录，会进行加锁操作。（悲观锁）

**快照读**：不加锁的select操作，读的是快照数据（历史版本），这个历史版本需要一个时间戳或者版本号进行判断。

**InnoDB实现MVCC:**

1. 添加三个**`隐藏字段`**：

	- `DB_TRX_ID（6字节）`：表示最后一次插入或更新该行的事务 id。此外，`delete` 操作在内部被视为更新，只不过会在记录头 `Record header` 中的`deleted_flag` 字段将其标记为已删除
	- `DB_ROLL_PTR（7字节）` 回滚指针，指向该行的 `undo log` 。如果该行未被更新，则为空
	- `DB_ROW_ID（6字节）`：如果没有设置主键且该表没有唯一非空索引时，`InnoDB` 会使用该 id 来生成聚簇索引

2. **ReadView(读视图):**

  说白了Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)。

   主要用来做**可见性判断**，将要被修改的数据的最新记录中的DB_TRX_ID（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果DB_TRX_ID跟Read View的属性做了某些比较，不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出Undo Log中的DB_TRX_ID再比较，即遍历链表的DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的DB_TRX_ID, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新老版本。

   我们可以把Read View简单的理解成有三个全局属性

  - **trx_list** 未提交事务ID列表，用来维护Read View生成时刻系统正活跃的事务ID
  - **up_limit_id** 记录trx_list列表中事务ID最小的ID
  - **low_limit_id** ReadView生成时刻系统尚未分配的下一个事务ID，也就是目前已出现过的事务ID的最大值+1

  那么Read View的工作流程如下

  1. 首先比较DB_TRX_ID < up_limit_id, 如果小于，则当前事务能看到DB_TRX_ID 所在的记录，如果大于等于进入下一个判断
  2. 接下来判断 DB_TRX_ID 大于等于 low_limit_id , 如果大于等于则代表DB_TRX_ID 所在的记录在Read View生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断
  3. 判断DB_TRX_ID 是否在活跃事务之中，trx_list.contains(DB_TRX_ID)，如果在，则代表我Read View生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的；如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，你修改的结果，我当前事务是能看见的

3. **Undo log：**

	InnoDB把这些为了回滚而记录的这些东西称之为undo log。这里需要注意的一点是，由于查询操作（SELECT）并不会修改任何用户记录，所以在查询操作执行时，并不需要记录相应的undo log。undo log主要分为3种：

	- **Insert undo log** ：插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。
	- **Update undo log**：修改一条记录时，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。
	- **Delete undo log**：删除一条记录时，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。
	- 删除操作都只是设置一下老记录的DELETED_BIT，并不真正将过时的记录删除。
	- 为了节省磁盘空间，InnoDB有专门的purge线程来清理DELETED_BIT为true的记录。为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view）;如果某个记录的DELETED_BIT为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。

	对MVCC有帮助的实质是**update undo log** ，undo log实际上就是存在rollback segment中旧记录链，它的执行流程如下：

	1. **比如一个有个事务插入persion表插入了一条新记录，记录如下，name为Jerry, age为24岁，隐式主键是1，事务ID和回滚指针，我们假设为NULL**

	2. 现在来了一个事务1对该记录的name做出了修改，改为Tom

		1. 在事务1修改该行(记录)数据时，数据库会先对该行加排他锁
		2. 然后把该行数据拷贝到undo log中，作为旧记录，既在undo log中有当前行的拷贝副本
		3. 拷贝完毕后，修改该行name为Tom，并且修改隐藏字段的事务ID为当前事务1的ID, 我们默认从1开始，之后递增，回滚指针指向拷贝到undo log的副本记录，既表示我的上一个版本就是它
		4. 事务提交后，释放锁

	3. 又来了个事务2修改person表的同一个记录，将age修改为30岁

		1. 在事务2修改该行数据时，数据库也先为该行加锁
		2. 然后把该行数据拷贝到undo log中，作为旧记录，发现该行记录已经有undo log了，那么最新的旧数据作为链表的表头，插在该行记录的undo log最前面
		3. 修改该行age为30岁，并且修改隐藏字段的事务ID为当前事务2的ID, 那就是2，回滚指针指向刚刚拷贝到undo log的副本记录
		4. 事务提交，释放锁

从上面，我们就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录（当然就像之前说的该undo log的节点可能是会purge线程清除掉，向图中的第一条insert undo log，其实在事务提交之后可能就被删除丢失了

### explain

#### type

1. null：没有使用索引
2. system、const：对查询某部分优化，并转换成常量
3. eq_ref：主键或唯一键索引被使用，只返回一条记录
4. ref：使用普通索引
5. range：范围查询
6. index：扫描全索引，一般比ALL快一点
7. ALL：全表扫描，没走索引

## Redis

> C语言开发的数据库，数据存在内存中，读写速度很快，广泛应用于缓存方向
>

### 简介

#### 工作流程

1. 用户发起请求
2. 查看缓存中是否有对应数据，if no turn to 3
3. 查看数据库中是否有对应数据， if no turn to 4
4. 返回空数据

#### 优点

高性能：直接在内存上操作当然快啦！

高并发：MySQL的QPS（服务器每秒可以执行的查询次数）大概在一万左右，而Redis轻松10w+，甚至可达30w

#### 用途

1. 分布式锁：Redlock方案
2. 限流：通过Redis + Lua脚本实现
3. 消息队列：类似于Kafka，有主题和消费组的概念，通过stream实现，支持消息持久化以及ACK机制
4. 复杂业务场景：以bitmap为例可以统计活跃用户

### 数据结构

#### 基本

1. String：key-value型，是**二进制安全**的，可以包含任何数据，且是**API安全的**，不会造成缓冲区溢出，常用于**常规数据的缓存**。
2. List：**双向链表**，常用于发布与订阅或者说消息队列、慢查询
3. Hash：**类似于JDK1.8前的hashmap**，常用于存储对象
4. Set：类似于**HashSet**，常用于实现求共同关注、共同粉丝、共同喜好等功能
5. Sorted Set：**在Set基础上增加权重参数score，也就是增加了排序功能**，常用于求实时的排行信息
6. bitmap：**只存储0、1，也就是二进制**，常用于保存是/否的状态信息

#### 重写String的原因

C语言通过字符数组表示字符串，在数组末尾加 '\0' 结尾，这显然不利于redis存储String类型的数据。在效率、安全性各个方面上都无法满足redis 的需求。重写的SDS的结构如下：

```
struct sdshdr {
    // 记录 buf 数组中已使用的数量
    unsigned int len; 
    // 记录 buf 中未使用的空间数量
    unsigned int free; 
    // 字符数组，用于保存字符串
    char buf[];  
};
```

**优势**

1. 通过len字段获取字符串长度的复杂度为O(1)
2. 通过free判断未使用的长度并与需要拼接的字符串做比较，可以避免拼接字符串时的缓冲区溢出问题
3. 实现空间预分配和空间惰性释放（对SDS扩展时会根据算法分配额外空间，缩减时并不会马上收回空间），减少修改字符串时带来的内存分配次数
4. 二进制安全，通过len来判断字符串的结尾

#### 跳表

主要是加快查询时间，减少遍历需要的元素个数

![img](笔记.assets\e61190ef76c6a7ef314419fa0f46b858f1de66d6-16518936804462.jpeg?msec=1657797480990)

跳表查询数据的时间复杂度为O(log(n))，空间复杂度为(O(n))

### 线程模型

#### 单线程模型

**基于Reactor模式设计高效事件处理模型，通过I/O多路复用程序来监听来自客户端的大量链接。**PS:使用I/O多路复用技术的好处在于不需要额外创建线程监听链接，降低资源消耗。

Redis服务器是事件驱动程序：1.文件事件、2.时间事件。在《Redis 设计与实现》有一段话是如是介绍文件事件的：

Redis 基于 Reactor 模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）。文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。**虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字**，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。

画个图的话大概长这样

![img](笔记.assets\redis事件处理器.66ac2f3d.png?msec=1657797480991)

#### 多线程引入

不用多线程的原因：

1. 单线程编程容易并且更容易维护；
2. Redis 的性能瓶颈不在 CPU ，主要在内存和网络；
3. 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。

使用多线程的原因：提高**网络IO读写性能**

### 过期相关

设置过期时间的用处：

1. 缓解内存消耗
2. 出于业务需要（比如一天内保持登录状态）

判断过期时间：通过过期字典保存数据过期事件，键指向Redis数据库中的key，值是long long类型整数，以毫秒精度时间戳保存键对应的数据库key过期时间

#### 删除策略

1. **惰性删除** ：只会在取出 key 的时候才对数据进行过期检查，但是可能会造成太多过期 key 没有被删除。（CPU友好)
2. **定期删除** ： 每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。(内存友好)

#### 内存淘汰机制

1. **volatile-lru（least recently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
2. **volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
3. **volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
4. **allkeys-lru（least recently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）
5. **allkeys-random**：从数据集（server.db[i].dict）中任意选择数据淘汰
6. **no-eviction**：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！

4.0 版本后增加以下两种：

1. **volatile-lfu（least frequently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰
2. **allkeys-lfu（least frequently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key

Redis使用的一般是近似的LRU，首先会给每个key增加一个额外的小字段，每个字段的长度是24个bit，填写最后一次被访问的时间戳。然后当内存超出maxmemory的时候，随机采样N个key，然后淘汰掉最旧的key，如果淘汰后还是超出maxmemory，那就继续随机采样淘汰，直到内存低于maxmemory为止。当然如果指定也会采用严格的LRU

### 持久化

**分为快照、只追加文件两种方式**

#### RDB(快照)

通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。是Redis默认采用的持久化方式

#### AOF(只追加文件)

与快照持久化相比，AOF持久化的实时性更好，因此已成为主流的持久化方案。默认情况下 Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启：

```conf
appendonly yes
```

开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入到内存缓存 `server.aof_buf` 中，然后再根据 `appendfsync` 配置来决定何时将其同步到硬盘中的 AOF 文件。

AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是 `appendonly.aof`。

在Redis 的配置文件中存在三种不同的AOF持久化方式，它们分别是：

```conf
appendfsync always    #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度
appendfsync everysec  #每秒钟同步一次，显式地将多个写命令同步到硬盘
appendfsync no        #让操作系统决定何时进行同步
```

为了兼顾数据和写入性能，用户可以考虑 `appendfsync everysec` 选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。

### 高可用->高可扩展

#### 主从复制

#### 哨兵机制

#### 分片技术

### BigKey

bigkey 顾名思义是 value 占用的内存空间比较大的key

#### 类型

1. 字符串类型:value 大小大于特点值比如 5KB 的string 类型的key。
2. 非字符串/集合类型: 比如存储非常多元素的hash， set ，zset，list类型，如本文案例的存储情况。

#### 查找方法

1. 使用Redis自带的**--bigkeys**参数查找（智能找出每种数据结构中的top 1 bigkey
2. 分析RDB文件找出big key

#### ，风险

1. 性能风险：Redis 单线程的工作机制，主线程处理所有key的增删改查。对bigkey的操作耗时增加将阻塞主线程处理其他业务请求，进而影响整体吞吐量。
2. 贷款风险：单个key 7MB ，如果每秒100次查询，则带来700MB 的带宽，虽然现在大部分是万兆网卡，业务请求量再大一些，网卡也有被打满的风险。
3. 数据倾斜：分片的redis集群，存在bigkey 会导致单个分片数据量远大于其他节点，整体不均衡。如果一个分片空间容量满了，对系统造成不可访问，而且也不能随意扩容，因为不拆分key的情况下扩容，单个分片还是存在数据倾斜。**更惨的是，数据量比较大，那么访问就增加，容易形成热点。热点不都是因为数据倾斜导致，数据倾斜会大概率导致热点。**
4. 主从同步风险：Redis Server 的输出大小通常是不可控制的。存在bigkey的时候，就会产生体积庞大的返回数据。另外也有可能因为执行了太多命令，导致产生返回数据的速率超过了往客户端发送的速率，导致服务器堆积大量消息，从而导致输出缓冲区越来越大，占用过多内存，甚至导致系统崩溃。Redis 通过设置client-output-buffer-limit 来保护系统安全。

#### 解决方法

**拆分**

1. 对于字符串类型的key，我们通常要在业务层面将value的大小控制在10KB左右，如果value确实很大，可以考虑采用序列化算法和压缩算法来处理，推荐常用的几种序列化算法:Protostuff、Kryo或者Fst。
2. 对于集合类型的key，我们通常要通过控制集合内元素数量来避免bigKey，通常的做法是将一个大的集合类型的key拆分成若干小集合类型的key来达到目的。

### 事务

**不支持roll back！，所以不满足原子性，因而也不满足持久性**

通过 **`MULTI`，`EXEC`，`DISCARD` 和 `WATCH`** 等命令来实现事务(transaction)功能。

大概理解为**Redis 事务提供了一种将多个命令请求打包的功能。然后，再按顺序执行打包的所有命令，并且不会被中途打断。**

### 缓存相关

#### 热key问题

> 在Redis中，我们把访问频率高的Key，称为热Key。比如突然又几十万的请求去访问redis中某个特定的Key，那么这样会造成redis服务器短时间流量过于集中，很可能导致redis的服务器宕机。那么接下来对这个Key的请求，都会直接请求到我们的后端数据库中，数据库性能本来就不高，这样就可能直接压垮数据库，进而导致后端服务不可用

**解决方法**

1. redis集群扩容，增加分片副本，分摊请求
2. 使用二级缓存，即使用redis和jvm本地缓存

#### 缓存穿透

> 问题描述：大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。

**解决方法：**

1. 做好参数校验：比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。

2. 缓存无效key：不能从根源上解决问题嘞，弟中之弟。大体意思是对黑客恶意攻击的请求key，缓存一下，过期时间设置的短一些。设置key的等效代码大概长这样
   
   ```java
   public Object getObjectInclNullById(Integer id) {
     // 从缓存中获取数据
     Object cacheValue = cache.get(id);
     // 缓存为空
     if (cacheValue == null) {
         // 从数据库中获取
         Object storageValue = storage.get(key);
         // 缓存空对象
         cache.set(key, storageValue);
         // 如果存储数据为空，需要设置一个过期时间(300秒)
         if (storageValue == null) {
             // 必须设置过期时间，否则有被攻击的风险
             cache.expire(key, 60 * 5);
         }
         return storageValue;
     }
     return cacheValue;
   }
   ```

3. 布隆过滤器：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话正常运行。（ps：**布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。**）

#### 缓存雪崩

指的是缓存在同一时间大面积的失效，后面的请求都直接落到了数据库上，造成数据库短时间内承受大量请求。**抑或是**有一些被大量访问数据（热点缓存）在某一时刻大面积失效，导致对应的请求直接落到了数据库上。**

解决方法：

1. 针对Redis服务不可用
   1. 采用 Redis 集群，避免单机出现问题整个缓存服务都没办法使用。
   2. 限流，避免同时处理大量的请求。
2. 针对热点缓存失效
   1. 设置不同的失效时间比如随机设置缓存的失效时间。
   2. 缓存永不失效

#### 缓存与数据库数据一致性保证

[详解如何实现一致性](https://mp.weixin.qq.com/s?__biz=MzIyOTYxNDI5OA==&mid=2247487312&idx=1&sn=fa19566f5729d6598155b5c676eee62d&chksm=e8beb8e5dfc931f3e35655da9da0b61c79f2843101c130cf38996446975014f958a6481aacf1&scene=178&cur_album_id=1699766580538032128#rd)

**概括性的总结**

1. 想要提高应用的性能，可以引入「缓存」来解决
2. 引入缓存后，需要考虑缓存和数据库一致性问题，可选的方案有：「更新数据库 + 更新缓存」、「更新数据库 + 删除缓存」
3. 更新数据库 + 更新缓存方案，在「并发」场景下无法保证缓存和数据一致性，且存在「缓存资源浪费」和「机器性能浪费」的情况发生
4. 在更新数据库 + 删除缓存的方案中，「先删除缓存，再更新数据库」在「并发」场景下依旧有数据不一致问题，解决方案是「延迟双删」，但这个延迟时间很难评估，所以推荐用「先更新数据库，再删除缓存」的方案
5. 在「先更新数据库，再删除缓存」方案下，为了保证两步都成功执行，需配合「消息队列」或「订阅变更日志」的方案来做，本质是通过「重试」的方式保证数据一致性
6. 在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」也会导致缓存和数据库不一致，缓解此问题的方案是「延迟双删」，凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率

讲讲**旁路缓存模式(Cache Aside Pattern)**

也是先更新DB，然后直接删除cache

1. **缓存失效时间变短（不推荐，治标不治本）** ：我们让缓存数据的过期时间变短，这样的话缓存就会从数据库中加载数据。另外，这种解决办法对于先操作缓存后操作数据库的场景不适用。
2. **增加 cache 更新重试机制（常用）**： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将缓存中对应的 key 删除即可。

## MongoDB

>本质是面向文档的NoSQL

## ElasticSearch

> 本质上是一个实时的分布式搜索分析引擎，主要功能有：**全文检索、结构化搜索、分析**

延申出主要的应用场景如下：

1. 网站搜索、垂直搜索、代码搜索
2. 日志管理与分析、安全指标监控、应用性能监控、Web抓取舆情分析

基于一套简单一致的RESTful API进行调用

### 核心概念

1. Near Realtime：近实时，数据提交索引后，马上可以搜索到
2. cluster：一个集群由一个唯一的名字标识，默认为“elasticSearch”，具有相同集群名节点才会组成一个集群
3. node：存储集群数据，参与集群的索引和搜索功能
4. index：索引是文档的集合
5. type：类型指一个索引中可以索引不同类型的文档（后面废弃了
6. document：文档指被索引的数据，是索引的基本信息单位
7. shard：分片，一个索引可以被指定分成多个分片来存储

>与传统RDBMS进行对比

| RDBMS              | ElasticSearch  |
| ------------------ | -------------- |
| database           | index          |
| table              | type           |
| row(行)            | document       |
| column(列)         | field          |
| schema(表结构)     | mapping(映射)  |
| 索引               | 反向索引       |
| SQL                | 查询DSL        |
| SELECT* FROM table | GET http://    |
| UPDATE table SET   | PUT http://    |
| DELETE             | DELETE http:// |

### 查询

#### 入门感受

1. GET /索引/类型/(具体查的话需要给id)
2. 查询所有（match_all关键字 ，sort进行排序）
3. 分页查询（from+size两个关键字）
4. 指定字段查询（match关键字）
5. 查询段落匹配（模糊查询：使用match_phrase关键字，只要字段中包含查询内容就会搜索到）
6. 多条件查询（bool关键字，可以组合多个查询条件，must代表某一条件是，must_not表示某一条件不是，使用filter可以达到类似的效果。但是前者会有个_score评分，匹配度越高该分值越高，后者则不会打分。还有term表示确定的值，range表示某一范围）

#### 复合查询

**bool query**

> 通过布尔逻辑将较小的查询组合成较大的查询

**特点：**

1. 子查询可以任意顺序出现
2. 可以嵌套多个查询，包括bool查询
3. 如果boo
4. 查询中没有must条件，should中必须至少满足一条才会返回结果

**参数：**（下面说的算分就是匹配度越高得分越高

1. must：必须匹配，贡献算分
2. must_not：过滤子句，必须不能匹配，但不贡献算分
3. shold：选择性匹配，至少满足一条，贡献算分
4. filter：过滤自居，必须匹配，但不贡献算分

这里引出一些更细节的概念：term查询代表完全匹配，精确查询，搜索前不会进行分词；而match查询会先对搜索词进行分词再进行匹配。match查询相当于模糊匹配，match_all更是能够匹配索引中所有文件

```sh
GET _search
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "term": {
          "status": "active"
        }
      }
    }
  }
}
```

以该栗子为例，所有文档指定1.0分

**boosting query**

> boosting query在不匹配时只是降低显示的优先级，不是不显示

**参数：**

1. positive：表示出现该词时显示优先级上升
2. negative：表示出现该词时显示优先级下降

```sh
GET /test-dsl-boosting/_search
{
  "query": {
    "boosting": {
      "positive": {
        "term": {
          "content": "apple"
        }
      },
      "negative": {
        "term": {
          "content": "pie"
        }
      },
      "negative_boost": 0.5
    }
  }
}
```

**constant_score**

> 查询某个条件时，返回指定的score。不需要计算scor可以用filter哦

**参数：**

1. boost：指定分数值

```sh
GET /test-dsl-constant/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "term": { "content": "apple" }
      },
      "boost": 1.2
    }
  }
}
```

**dis_max**

> 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回

**参数：**

1. tie_breaker：这个参数决定第二个条件分数值的权重

```sh
GET /test-dsl-dis-max/_search
{
    "query": {
        "dis_max": {
            "queries": [
                { "match": { "title": "Brown fox" }},
                { "match": { "body":  "Brown fox" }}
            ],
            "tie_breaker": 0
        }
    }
}
```

像这里最终得到

分数就是：第一个匹配条件分数 + tie_breaker * 第二个匹配的条件的分数

**function_score**

> 自定义计算算分(_score)的方式

**参数：**

1. script_score：完全自定义分值计算逻辑
2. weight：权重嘛，乘上这个数值就行
3. random_score：一致性随机分值计算，对每个用户采用不同结果排序
4. field_value_factor：使用文档中某个字段值改变_score

```sh
GET /_search
{
  "query": {
    "function_score": {
      "query": { "match_all": {} },
      "boost": "5",
      "random_score": {}, 
      "boost_mode": "multiply"
    }
  }
}
```

这个就是random_score的使用实例

#### 全文搜索

**match类型：**

```sh
GET /test-dsl-match/_search
{
    "query": {
        "match": {
            "title": "QUICK!"
        }
    }
}
```

以执行这个查询为例，步骤如下

1. **检查字段类型：**title是一个string类型已分析的全文字段，意味着查询字符串本身也应该被解析
2. **分析查询字符串：**将查询字符串传入标准分析器中，输出结果为单个项quick，因为只有一个单词项，所以match查询执行的是单个底层term查询
3. **查找匹配文档：**用term查询在倒排索引中查找quick，获取一组包含该项的文档
4. **为每个文档评分：**用 term 查询计算每个文档相关度评分 _score ，这是种将词频（term frequency，即词 quick 在相关文档的 title 字段中出现的频率）和反向文档频率（inverse document frequency，即词 quick 在所有文档的 title 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式。

不难看出，match查询的本质其实还是term查询，如果是查询多个，可以等价于某一个bool查询，当我们要求至少匹配查询词中的几个时，可以设置参数`minimum_should_match`，其值为一个百分数，表示至少和给出查询词的百分之多少匹配。如果你选择给出一个具体的值，应该使用should参数。

谈一嘴`match_pharse`，相当于连续的term查询，在文档中必须要与关键词连续匹配才行

#### 词项搜索

列几个常见的参数

exist：查找是否存在字段；

ids：根据id查找；

prefix：通过前缀查找

term：根据分词查询

terms：根据多个分词查询，or关系

term set：用文档中数字字段动态匹配查询满足term的个数

dildcard：通配符匹配，可以用*

range：下分gte、lte参数，表示某一范围内

regexp：通过正则表达式查询

fuzzy：模糊匹配，允许一定编辑距离可以得到的词与关键词匹配

### 聚合

#### 入门感受

其实聚合的本质也是查询，相当于使用了group by的sql

简单聚合（不进行分词统计，使用aggs关键字进行聚合）

嵌套聚合（聚合+聚合，在之前的聚合结果上继续聚合，可以使用order关键字进行排序）

#### 桶聚合

- **桶（Buckets）** 满足特定条件的文档的集合
- **指标（Metrics）** 对桶内的文档进行统计计算

桶类似于group by，指标类似于count()、sum()等统计方法

#### 指标聚合

- **从分类看**：Metric聚合分析分为**单值分析**和**多值分析**两类
- **从功能看**：根据具体的应用场景设计了一些分析api, 比如地理位置，百分数等等

#### 管道聚合

> 让上一步的聚合结果成为下一个聚合的输入，有点像责任链模式哈

### 索引模板

在建立索引的过程中我们可以进行控制，在索引创建后我们也可以进行管理。比方说在创建索引时传入设置或者类型映射，或者禁止自动创建索引，或者手动创建索引。**索引模板就是创建索引时配置索引的方法，是将创建好的某个索引的参数设置和索引映射保存下载作为模板，为快速构建和管理索引而生**

索引模板分为两种类型：索引模板和组件模板。组件模板是可重用的构建块，用于配置映射，设置和别名；索引模板是组件模板的集合。**介绍下模板中的一些参数：index_patterns代表匹配的索引、settings指索引层面的设置**

### 数据结构

#### Inverted Index

> 从本质上说，ElasticSearch是基于Lucene工作的，Lucene是一个Full Text搜索库。

**Iverted Index：**Lucene中最重要的数据结构，由一个字典+一个对应的文件组成

- 一个有序的数据字典Dictionary（包括单词Term和它出现的频率）。
- 与单词Term对应的Postings（即存在这个单词的文件）。

搜索时，先对搜索内容分解，在字典找到对应Term，可以查找到与搜索内容相关文件。下面上点官方给出的图解帮助消化

- **自动补全**（AutoCompletion-Prefix）

如果想要查找以字母“c”开头的字母，可以简单的通过二分查找（Binary Search）在Inverted Index表中找到例如“choice”、“coming”这样的词（Term）。

- **昂贵的查找**

如果想要查找所有包含“our”字母的单词，那么系统会扫描整个Inverted Index，这是非常昂贵的。

在此种情况下，如果想要做优化，那么我们面对的问题是如何生成合适的Term。

- **问题的转化**

对于以上诸如此类的问题，我们可能会有几种可行的解决方案：

1. `* suffix -> xiffus *`

如果我们想以后缀作为搜索条件，可以为Term做反向处理。

1. `(60.6384, 6.5017) -> u4u8gyykk`

对于GEO位置信息，可以将它转换为GEO Hash。

1. `123 -> {1-hundreds, 12-tens, 123}`

对于简单的数字，可以为它生成多重形式的Term。

- **解决拼写错误**

一个Python库 为单词生成了一个包含错误拼写信息的树形状态机，解决拼写错误的问题。

#### Stored Field

> 实现字段查找

本质上，Stored Fields是一个简单的键值对key-value。

默认情况下，ElasticSearch会存储整个文件的JSON source。

#### Document Values

> 实现排序、聚合

本质上就是一个列式的存储，它高度优化了具有相同类型的数据的存储结构。查资料可知，其为Lucene在构建倒排索引时。额外建立的有序正排索引，默认对字符串类型是不起作用的。其实就是一列存储一类数据嘞

为了提高效率，ElasticSearch可以将索引下某一个Document Value全部读取到内存中进行操作，这大大提升访问速度，但是也同时会消耗掉大量的内存空间。

### 搜索过程

先讲讲Lucene的一些特性：

1. Segments不可变
   1. 删除：只会将标志位置标记为del，但是文件不会发生改变
   2. 更新：先删除后，重新索引
2. 压缩
3. 缓存所有的信息

### 倒排

> 倒排就是指倒排索引，将文档中的每个词映射到包含该词的文档列表中。在传统的索引结构中，是根据文档来查找词，而倒排索引则是通过词来查找文档，这样可以更快地进行搜索操作。下面关于倒排索引的有点点阐释来自chatgpt

倒排索引之所以能够快速进行搜索，主要有以下几个原因：

1. 减少搜索范围：倒排索引将每个词映射到包含该词的文档列表中。当进行搜索时，可以根据关键词快速定位到包含该词的文档，从而减少了需要搜索的文档范围。相比于传统的索引结构，倒排索引可以更快地缩小搜索范围，提高搜索效率。

2. 跳跃指针：在倒排索引中，文档列表一般是按照文档的ID进行排序的。通过使用跳跃指针，可以在搜索过程中快速跳过不需要的文档，只关注与查询相关的文档。这种跳跃指针的机制可以减少不必要的遍历，提高搜索速度。

3. 压缩和编码：倒排索引通常会对文档列表进行压缩和编码，以减少存储空间和提高读取速度。通过使用压缩算法和编码方式，可以在保证搜索准确性的前提下，减少磁盘IO和网络传输的开销，提高搜索效率。

4. 内存缓存：倒排索引通常会将部分或全部的索引数据加载到内存中进行缓存。由于内存的读取速度比磁盘快很多，因此可以通过内存缓存来加速搜索操作。当搜索请求到达时，可以首先在内存中进行搜索，只有在内存中没有找到时才会访问磁盘。

综上所述，倒排索引通过减少搜索范围、跳跃指针、压缩和编码以及内存缓存等技术手段，实现了快速搜索的能力。这些优化措施可以减少不必要的遍历和IO操作，提高搜索效率，使得倒排索引成为一种高效的搜索数据结构。
